What You Have ✅ 1. Self-refinement system (enhanced-proposer-service.ts:265-373, proposer-refinement-rules.ts) - 3-cycle TypeScript error detection/fixing 2. Failure classification (failure-classifier.ts) - Structured error categorization 3. Learning sample creator (learning-sample-creator.ts) - Captures complexity calibration data 4. Contract validation - Integrated into refinement loop Critical Gaps Identified ❌ 1. NO code sanitizer for generated TypeScript - learning from mechanical syntax issues (smart quotes, unquoted modules) instead of semantic problems 2. NO prompt enhancement database - Zero tables for proposer_failures, prompt_enhancements, prompt_versions 3. NO meta-AI learning loop - Cannot auto-improve prompts based on effectiveness 4. NO production hardening - Missing sampling, rolling windows, threshold experiments --- Recommended Action Plan (Prioritized) PHASE 1: IMMEDIATE CODE QUALITY FIX (1-2 hours) Priority: CRITICAL - Document explicitly says: "The sanitizer should be implemented FIRST to immediately improve quality" Task 1.1: Create TypeScript Code Sanitizer File: src/lib/code-sanitizer.ts Integration: Add to refinement loop BEFORE TypeScript checking (proposer-refinement-rules.ts:54-91) // Pre-process generated code to fix mechanical issues const { sanitized, changes_made } = sanitizeTypeScript(code); if (changes_made.length > 0) { console.log(🧹 Auto-fixed: ${changes_made.join(', ')}); code = sanitized; } // NOW check TypeScript errors on sanitized code const errors = await checkTypeScriptErrors(code); Fixes to implement: - Smart quotes (""'' → "'") - Unquoted module declarations (declare module foo → declare module 'foo') - Em/en-dashes (—– → -) - Unclosed template literals (heuristic detection) Why first: Prevents refinement cycles wasting tokens on syntax issues that can be mechanically fixed. --- PHASE 2: LEARNING PIPELINE FOUNDATION (4-6 hours) Task 2.1: Create Supabase Schema Files to create: - supabase/migrations/YYYYMMDDHHMMSS_proposer_learning_system.sql Tables (6 total): 1. proposer_failures - Log residual errors after refinement 2. prompt_enhancements - Store error-specific prompt improvements 3. proposer_success_metrics - Track proposer performance by complexity band 4. proposer_attempts - Rolling window of recent attempts (Component 6) 5. prompt_versions - Versioned prompt registry (Component 7) 6. threshold_experiments - A/B testing for threshold increases (Component 8) Task 2.2: Implement Failure Logger File: src/lib/proposer-failure-logger.ts Integration point: enhanced-proposer-service.ts:336-367 (where failure_class is determined) // After refinement completes with residual errors if (refinementMetadata && refinementMetadata.final_errors > 0) { await logProposerFailure({ work_order_id: request.metadata?.work_order_id, proposer_name: proposerName, complexity_score: complexityAnalysis.score, initial_errors: refinementMetadata.initial_errors, final_errors: refinementMetadata.final_errors, error_codes: refinementMetadata.error_details.map(e => e.code), error_samples: refinementMetadata.error_details.slice(0, 5), refinement_count: refinementMetadata.refinement_count, sanitizer_changes: sanitizerMetadata.changes_made // Track what was auto-fixed }); } Sampling: Log all failures (100%), sample successes (10%) - Component 5 --- PHASE 3: META-AI LEARNING LOOP (6-8 hours) Priority: HIGH - The transformative component Task 3.1: Prompt Enhancement Analyzer File: src/lib/prompt-enhancement-analyzer.ts Functions: - analyzeEnhancementEffectiveness() - Measure error reduction rates per enhancement - generateImprovedEnhancement() - Use Claude Sonnet 4.5 to rewrite ineffective prompts - generateNewEnhancement() - Create prompts for uncovered error patterns - runAutoImprovement() - Weekly cron job Key logic: // For each enhancement, compare error rates WITH vs WITHOUT it const errorFreqWith = countErrorCode(withEnhancement, 'TS1443'); const errorFreqWithout = countErrorCode(withoutEnhancement, 'TS1443'); const reduction_rate = (errorFreqWithout - errorFreqWith) / errorFreqWithout; // If reduction < 50%, ask Claude to rewrite the enhancement if (reduction_rate < 0.5) { const improved = await generateImprovedEnhancement({ error_code: 'TS1443', current_text: "Always quote module declarations", recent_failures: [...sample error messages...] }); // Update database with improved prompt } Task 3.2: Prompt Injection System File: src/lib/prompt-enhancer.ts Integration: enhanced-proposer-service.ts:670-687 (buildClaudePrompt / buildOpenAIPrompt) private async buildClaudePrompt(request: ProposerRequest): Promise<string> { const basePrompt = Task: ${request.task_description}...; // NEW: Inject error-specific enhancements const enhancements = await getActiveEnhancements(request.expected_output_type); const enhancementText = enhancements.map(e => e.enhancement_text).join('\n\n'); return ${basePrompt}\n\n${enhancementText ? IMPORTANT GUIDELINES:\n${enhancementText} : ''}; } --- PHASE 4: PRODUCTION HARDENING (4-6 hours) Task 4.1: Rolling Windows (Component 6) Current issue: learning-sample-creator.ts captures all history → early bad performance permanently hurts metrics Solution: Use proposer_attempts table with 50-record sliding window per complexity band Task 4.2: Automatic Threshold Experiments (Component 8) File: src/lib/adaptive-threshold-experiments.ts Integration: Weekly cron job + proposer-executor completion hook // After each WO completes await recordExperimentResult(proposerName, complexityScore, wasSuccess); // System automatically: // - Tracks 30 attempts in experimental range // - Promotes threshold if success rate ≥ 70% // - Rejects and keeps old threshold otherwise --- Implementation Priority Matrix | Component | Impact | Effort | Priority | Timeline | |-----------------------|---------------|--------|----------|----------| | Code Sanitizer (9) | 🔥 High | 2h | NOW | Day 1 AM | | Database Schema | 🔥 High | 3h | Phase 2 | Day 1 PM | | Failure Logger (5) | 🔥 High | 3h | Phase 2 | Day 1 PM | | Meta-AI Loop (10) | 🔥🔥 Critical | 8h | Phase 3 | Day 2-3 | | Prompt Injection | 🔥 High | 2h | Phase 3 | Day 3 | | Rolling Windows (6) | Medium | 4h | Phase 4 | Week 2 | | Threshold Tests (8) | Medium | 4h | Phase 4 | Week 2 | | Versioned Prompts (7) | Low | 3h | Phase 5 | Week 3 | --- Success Metrics (after 100 WOs) 1. TS1443 reduction: 50%+ drop (or move to sanitizer-fixed category) 2. Refinement success rate: 70%+ for complexity 0.3-0.4 band 3. Auto-generated enhancements: System creates useful prompts for new error patterns within 24h 4. Database growth: <10MB per 1000 WOs (via 10% success sampling) --- Critical Implementation Notes Integration Points (read before implementing) 1. Sanitizer: proposer-refinement-rules.ts:54 (before checkTypeScriptErrors) 2. Failure logger: enhanced-proposer-service.ts:337 (after refinement metadata calculated) 3. Prompt enhancer: enhanced-proposer-service.ts:670, 680 (buildClaudePrompt/buildOpenAIPrompt) 4. Experiment tracker: proposer-executor.ts (after WO completion) §5.1 Compliance Checkpoint - ✅ All proposed changes preserve existing ground truth - ✅ Schema creation requires Supabase review before execution - ✅ Meta-AI loop uses Claude Sonnet 4.5 (not experimental models) - ✅ Sanitizer changes are logged (not hidden from learning system) --- Recommended First Steps (Next 4 Hours) Hour 1: Implement code sanitizer + integrate into refinement loopHour 2: Create Supabase migration (6 tables) + test locallyHour 3: Implement failure logger + wire into proposer serviceHour 4: Validation test - trigger failures, verify logging works This gives you immediate quality improvement (sanitizer) + data collection pipeline. The meta-AI loop (Phase 3) then has data to analyze and improve prompts automatically. ● Ready to proceed? I recommend starting with Phase 1 (code sanitizer) immediately - it's a 2-hour task that will reduce refinement cycles wasted on mechanical syntax issues. Should I begin implementation?