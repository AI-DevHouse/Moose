====================================================================
COMPLEXITY SCORING & ROUTING ANALYSIS
Claude Response - Based on System Inspection
Date: 2025-10-11
====================================================================

EXECUTIVE SUMMARY
-----------------
The routing logic IS over-scoring complexity. With avg complexity 0.81
and GPT-4o-mini threshold at 0.3, nearly all tasks (>95%) exceed the
threshold and route to Claude 4.5. This defeats the cost optimization
goal.

The previous discussion correctly identified the issues but the
recalibration hasn't been applied yet.


ACTUAL SYSTEM STATE
-------------------
Database Thresholds:
  - gpt-4o-mini:        0.3 (max ceiling)
  - claude-sonnet-4-5:  1.0 (max ceiling)

Current Complexity Scores:
  - Average:    0.81
  - Range:      0.33 to 0.98
  - WOs scored: 32 out of 81

Model Usage (executed WOs):
  - Claude 4.5:   23 WOs (28.4%)
  - GPT-4o-mini:  8 WOs (9.9%)
  - Ratio:        3:1 Claude preference

Code State (complexity-analyzer.ts):
  - Baselines are HIGH:
    * Code complexity:   0.1 baseline
    * Security:          0.2 baseline
    * Architectural:     0.2 baseline
    * Reasoning depth:   0.3 baseline  ← HIGHEST
  - Additive patterns (no diminishing returns)
  - No pattern overlap normalization


ROOT CAUSE ANALYSIS
-------------------
1. BASELINE INFLATION (Primary Issue)
   - Before any pattern matching, scores start at 0.8 (sum of baselines)
   - Reasoning depth alone starts at 0.3 (30% complexity floor)
   - This means EVERY task starts at medium-high complexity

2. THRESHOLD MISMATCH
   - GPT-4o-mini threshold: 0.3
   - Minimum possible score: ~0.4 (after baselines)
   - Result: GPT-4o-mini can ONLY handle tasks with ZERO patterns
   - This is unrealistic for actual work orders

3. ADDITIVE PATTERN STACKING
   - Task: "Implement authentication API integration"
   - Triggers:
     * "authentication" → security +0.3
     * "api" → code complexity +0.15
     * "integration" → code complexity +0.15
     * "implement" → reasoning +0.15
   - Total: 0.8 baseline + 0.75 patterns = 1.0+ (capped at 1.0)
   - This task could be GPT-4o-mini viable but scores maximum

4. MEMORY PRESSURE BIAS
   - Uses gpt-4o-mini's 128K context as pressure baseline (line 300)
   - Conservative USABLE_CONTEXT_RATIO of 0.8 (80%)
   - This correctly identifies context limits but...
   - ...combined with other high baselines, pushes scores up further


SPECIFIC CODE ISSUES (complexity-analyzer.ts)
---------------------------------------------
Line 196: `let score = 0.1;` // Code complexity baseline
Line 228: `let score = 0.2;` // Security baseline
Line 244: `let score = 0.2;` // Architectural baseline
Line 264: `let score = 0.3;` // Reasoning depth baseline ← WORST OFFENDER

Lines 197-199: Additive patterns without overlap normalization
```typescript
complexityIndicators.forEach(({ pattern, weight }) => {
  if (pattern.test(description)) score += weight;
});
```

Issue: If description matches 3 patterns, all weights add fully.
Better: Diminishing returns (60-70% weight for subsequent matches)


ROUTING LOGIC VERIFICATION (manager-routing-rules.ts)
-----------------------------------------------------
Line 139: `return complexityScore <= proposer.complexity_threshold;`

This is CORRECT - it treats threshold as max ceiling.
The problem is NOT the routing logic - it's the scoring inputs.

With current state:
  - 0.33 (lowest score) exceeds 0.3 threshold → Claude
  - 0.81 (average) exceeds 0.3 threshold → Claude
  - Only tasks scoring 0.0-0.3 get GPT-4o-mini
  - But baselines alone = 0.4-0.8 → impossible to score below 0.3


COMPARISON WITH DISCUSSION DOCUMENT
------------------------------------
The previous analysis identified the same issues:
  ✓ Baseline inflation
  ✓ Additive pattern stacking
  ✓ No diminishing returns
  ✓ Weight distribution problems

However, the proposed recalibration (in discussion doc) was NOT applied.
Current code still has original high baselines.


RECOMMENDATIONS
---------------

OPTION A: Apply Proposed Recalibration (Quick Fix - 15 min)
  - Reduce baselines:
    * reasoning: 0.3 → 0.1
    * security: 0.2 → 0.05
    * architectural: 0.2 → 0.05
    * code: 0.1 → 0.05
  - Add diminishing returns (60% weight for 2nd+ matches)
  - Expected: Avg complexity 0.81 → 0.35-0.45

OPTION B: Recalibrate Thresholds Only (Ultra-Quick - 2 min)
  - Keep current scoring
  - Raise GPT-4o-mini threshold: 0.3 → 0.7
  - This matches actual score distribution
  - Expected: GPT usage 9.9% → 40-50%

OPTION C: Hybrid Approach (Balanced - 20 min)
  - Moderate baseline reduction:
    * reasoning: 0.3 → 0.15
    * security: 0.2 → 0.1
    * others: keep same
  - Raise threshold: 0.3 → 0.45
  - Add pattern diminishing returns
  - Expected: Avg 0.81 → 0.50-0.60, better distribution


RECOMMENDED ACTION: Option C (Hybrid)
--------------------------------------
Why:
  1. Less drastic than full recalibration (lower risk)
  2. Addresses both scoring AND threshold mismatch
  3. Maintains scoring sensitivity for genuine complexity
  4. Can fine-tune further based on real data

Implementation:
  1. Edit complexity-analyzer.ts (lines 196, 228, 244, 264)
  2. Add diminishing returns loop (lines 197-199)
  3. Update database: gpt-4o-mini threshold 0.3 → 0.45
  4. Test on 10 sample WOs
  5. Monitor for 50-100 WOs
  6. Adjust if needed


EMPIRICAL VALIDATION PLAN
--------------------------
After applying changes, track:
  1. GPT-4o-mini usage % (target: 40-50%)
  2. GPT-4o-mini failure rate (target: <15%)
  3. Average cost per WO (target: 30-40% reduction)
  4. Fallback triggers (GPT → Claude, target: <10%)

Use existing getRoutingAccuracyByComplexity() function to analyze
routing performance by complexity band after 100+ WOs.


SPECIFIC CODE CHANGES (Hybrid Approach)
----------------------------------------

File: src/lib/complexity-analyzer.ts

Change 1 (Line 196):
  BEFORE: let score = 0.1; // baseline
  AFTER:  let score = 0.05; // baseline

Change 2 (Line 197-199):
  BEFORE:
    complexityIndicators.forEach(({ pattern, weight }) => {
      if (pattern.test(description)) score += weight;
    });

  AFTER:
    const matches = complexityIndicators.filter(({ pattern }) =>
      pattern.test(description)
    );
    matches.forEach(({ weight }, idx) => {
      const multiplier = idx === 0 ? 1.0 : 0.6;
      score += weight * multiplier;
    });

Change 3 (Line 228):
  BEFORE: let score = 0.2; // baseline
  AFTER:  let score = 0.1; // baseline

Change 4 (Line 244):
  BEFORE: let score = 0.2; // baseline
  AFTER:  let score = 0.1; // baseline

Change 5 (Line 264):
  BEFORE: let score = 0.3; // baseline
  AFTER:  let score = 0.15; // baseline

Apply similar diminishing returns to security (221-231) and
architectural (236-247) pattern loops.


Database Update:
  UPDATE proposer_configs
  SET complexity_threshold = 0.45
  WHERE name = 'gpt-4o-mini';


EXPECTED OUTCOMES
-----------------
Before Changes:
  - Avg complexity: 0.81
  - GPT usage: 9.9% (8/81 WOs)
  - Cost efficiency: Low (overusing expensive model)

After Hybrid Changes:
  - Avg complexity: 0.50-0.60 (estimated)
  - GPT usage: 35-45% (estimated)
  - Cost reduction: 30-35% (estimated)
  - Tasks 0.0-0.45: GPT-4o-mini
  - Tasks 0.45-1.0: Claude 4.5

This better balances cost vs capability.


RISK MITIGATION
----------------
Concern: GPT-4o-mini handles complex task poorly
Mitigation:
  - Monitor failure rates closely
  - Fallback logic already exists (enhanced-proposer-service.ts)
  - Can raise threshold further if needed (0.45 → 0.5)

Concern: Genuine complex tasks get under-scored
Mitigation:
  - Hard stop keywords still force Claude for security/architecture
  - Scores >0.6 still route to Claude
  - Context pressure still detected (memory pressure factor)


NEXT STEPS
----------
1. [ ] Apply hybrid code changes (20 min)
2. [ ] Update gpt-4o-mini threshold in DB (2 min)
3. [ ] Run npm run build && npm test (3 min)
4. [ ] Test on 10 existing WOs, verify score reduction (10 min)
5. [ ] Deploy and monitor 100 WOs (1-2 days)
6. [ ] Analyze routing accuracy data (15 min)
7. [ ] Fine-tune threshold if needed (5 min)


CONCLUSION
----------
The routing logic itself is sound, but complexity scoring is inflated
due to high baselines and additive pattern matching. The hybrid
approach provides the best balance of quick fixes and maintainable
scoring logic.

Current state is costing 3-4x more than optimal due to over-routing
to Claude 4.5 for tasks that GPT-4o-mini could handle.

====================================================================
End of Analysis
====================================================================
