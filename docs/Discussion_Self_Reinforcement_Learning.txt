# Moose Iterative Improvement System - Implementation Guide

**Document Purpose:** This document provides complete specifications for implementing an autonomous iterative improvement system for Moose Mission Control. Share this with Claude Code to implement the system.

**Date:** October 9, 2025  
**Context:** Moose is production-deployed and working. Now we need to validate and improve its quality by repeatedly building the same test application.

---

## 1. Executive Summary

### The Goal

Run an iterative improvement loop where:
1. Moose builds "Multi-LLM Discussion" app (first test application)
2. System scores the quality of the built app (1-10 scale)
3. System analyzes what went wrong and why
4. System proposes specific improvements to Moose's code
5. **Human reviews and approves proposals** (supervised mode initially)
6. System applies approved changes to Moose
7. Repeat with improved Moose until quality target is met (8/10 for 3 consecutive iterations)

### Key Principle: Human-in-the-Loop Initially

- Start in **SUPERVISED MODE** - human approves all changes
- Generate detailed reports showing exactly what will be changed
- After ~5 successful iterations, offer to switch to autonomous mode
- Can always stop, skip, or edit proposals

### Success Criteria

- Overall quality score >= 8/10 for 3 consecutive iterations
- Build succeeds, tests pass, isolation verified
- Code is production-ready

---

## 2. Architecture Overview

```
┌──────────────────────────────────────────────────────────────┐
│  SUPERVISED IMPROVEMENT LOOP                                  │
│                                                               │
│  ┌─────────┐  ┌─────────┐  ┌──────────┐  ┌───────────────┐ │
│  │ Execute │─►│  Score  │─►│ Analyze  │─►│ Generate      │ │
│  │ Build   │  │ Quality │  │ Failures │  │ Proposals     │ │
│  └─────────┘  └─────────┘  └──────────┘  └───────┬───────┘ │
│                                                     │         │
│                                              Create Report    │
│                                                     │         │
│                                                     ▼         │
│                                          ┌──────────────────┐│
│                                          │  WAIT FOR HUMAN  ││
│                                          │  APPROVAL        ││
│                                          └────────┬─────────┘│
│                                                   │          │
│                          ┌────────────────────────┼─────┐    │
│                          │                        │     │    │
│                     [Approve]  [Edit]  [Skip]  [Stop]  │    │
│                          │        │       │       │     │    │
│                          ▼        ▼       ▼       ▼     │    │
│                    ┌─────────┐   │   Next    Exit      │    │
│                    │  Apply  │◄──┘   Iter               │    │
│                    │ Changes │                          │    │
│                    └─────────┘                          │    │
│                          │                              │    │
│                          └──► Cleanup & Next Iteration ─┘    │
│                                                               │
└──────────────────────────────────────────────────────────────┘
```

### Local Execution Benefits

Running locally (not cloud-deployed) is BETTER for testing because:
- ✅ Aider runs on local machine (same as production)
- ✅ Git operations work identically
- ✅ Can watch files change in real-time
- ✅ Easy debugging and manual intervention
- ✅ No deployment delays or cloud costs
- ✅ Full control over the process

Deploy to cloud **later** after validation.

---

## 3. Database Schema

### New Tables Needed

```sql
-- ============================================================================
-- Table 1: test_iterations
-- Stores metrics for each iteration (NEVER deleted - this is learning data)
-- ============================================================================

CREATE TABLE test_iterations (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  iteration_number INTEGER NOT NULL,
  project_name TEXT NOT NULL, -- "multi-llm-discussion"
  moose_version TEXT, -- Git commit hash
  started_at TIMESTAMP NOT NULL DEFAULT NOW(),
  completed_at TIMESTAMP,
  status TEXT CHECK (status IN ('running', 'completed', 'failed')),
  
  -- Execution Metrics
  total_work_orders INTEGER,
  work_orders_succeeded INTEGER,
  work_orders_failed INTEGER,
  total_execution_time_seconds INTEGER,
  total_cost_usd DECIMAL(10, 4),
  
  -- Quality Metrics (filled after manual/automated review)
  builds_successfully BOOLEAN,
  tests_pass BOOLEAN,
  lint_errors INTEGER,
  human_quality_score INTEGER CHECK (human_quality_score BETWEEN 1 AND 10),
  
  -- Isolation Verification
  moose_files_modified BOOLEAN, -- Should always be FALSE
  isolation_verified BOOLEAN,
  
  -- Detailed scoring (JSON)
  scoring_details JSONB, -- Full breakdown of all scores
  analysis_details JSONB, -- Full analysis from Claude Code
  
  -- Notes
  what_worked TEXT,
  what_failed TEXT,
  improvements_made TEXT,
  
  created_at TIMESTAMP DEFAULT NOW()
);

-- ============================================================================
-- Table 2: iteration_work_order_logs
-- Detailed logs for each work order execution (linked to iteration)
-- ============================================================================

CREATE TABLE iteration_work_order_logs (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  iteration_id UUID REFERENCES test_iterations(id) ON DELETE CASCADE,
  work_order_id UUID REFERENCES work_orders(id) ON DELETE SET NULL,
  execution_order INTEGER, -- 1st, 2nd, 3rd WO executed
  
  -- Execution details
  started_at TIMESTAMP,
  completed_at TIMESTAMP,
  duration_seconds INTEGER,
  status TEXT,
  
  -- What happened
  files_modified TEXT[], -- Array of file paths
  lines_added INTEGER,
  lines_deleted INTEGER,
  pr_url TEXT,
  pr_merged BOOLEAN,
  
  -- AI/Aider details
  proposer_used TEXT, -- "claude-sonnet-4-5"
  aider_model TEXT, -- "claude-sonnet-4-5"
  prompt_tokens INTEGER,
  completion_tokens INTEGER,
  cost_usd DECIMAL(10, 4),
  
  -- Quality
  compilation_errors TEXT,
  test_results JSONB, -- { passed: 5, failed: 2, skipped: 1 }
  
  created_at TIMESTAMP DEFAULT NOW()
);

-- ============================================================================
-- Table 3: moose_improvements
-- Tracks what changes were made to Moose between iterations
-- ============================================================================

CREATE TABLE moose_improvements (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  from_iteration_id UUID REFERENCES test_iterations(id),
  to_iteration_id UUID REFERENCES test_iterations(id),
  
  improvement_type TEXT CHECK (improvement_type IN (
    'bug_fix', 'feature_add', 'prompt_tuning', 'architecture_change', 'config_change'
  )),
  
  description TEXT NOT NULL,
  files_changed TEXT[],
  git_commit_hash TEXT,
  
  expected_impact TEXT, -- "Should reduce WO failures by 20%"
  actual_impact TEXT,   -- "Actually reduced by 15%" (filled after next iteration)
  
  -- Proposal details (JSON)
  proposal_details JSONB, -- Full proposal that was approved
  
  created_at TIMESTAMP DEFAULT NOW()
);

-- ============================================================================
-- Helper Functions
-- ============================================================================

-- Function to cleanup orphaned proposals
CREATE OR REPLACE FUNCTION cleanup_orphaned_proposals()
RETURNS void AS $$
BEGIN
  DELETE FROM proposals
  WHERE work_order_id NOT IN (SELECT id FROM work_orders);
END;
$$ LANGUAGE plpgsql;

-- Function to cleanup orphaned execution logs
CREATE OR REPLACE FUNCTION cleanup_orphaned_execution_logs()
RETURNS void AS $$
BEGIN
  DELETE FROM execution_logs
  WHERE work_order_id NOT IN (SELECT id FROM work_orders);
END;
$$ LANGUAGE plpgsql;

-- Function to get cleanup status
CREATE OR REPLACE FUNCTION get_cleanup_status(test_project_name TEXT)
RETURNS TABLE (
  item TEXT,
  count BIGINT,
  should_be_zero BOOLEAN
) AS $$
BEGIN
  RETURN QUERY
  SELECT 'Projects'::TEXT, COUNT(*)::BIGINT, true
  FROM projects WHERE name = test_project_name
  UNION ALL
  SELECT 'Work Orders'::TEXT, COUNT(*)::BIGINT, true
  FROM work_orders 
  WHERE project_id IN (SELECT id FROM projects WHERE name = test_project_name)
  UNION ALL
  SELECT 'Orphaned Proposals'::TEXT, COUNT(*)::BIGINT, true
  FROM proposals
  WHERE work_order_id NOT IN (SELECT id FROM work_orders);
END;
$$ LANGUAGE plpgsql;

-- ============================================================================
-- Apply this schema to Supabase
-- ============================================================================
-- Run in Supabase SQL Editor:
-- 1. Copy all SQL above
-- 2. Paste into SQL Editor
-- 3. Click "Run"
-- 4. Verify tables created in Table Editor
```

---

## 4. Scoring Rubrics (Objective Quality Measurement)

### Philosophy

**Bad scoring:** "This looks pretty good... I guess 7?"  
**Good scoring:** "It has 2 issues from the '8' criteria, so it's a 6."

Each score level has **specific, observable criteria**. You're checking boxes, not guessing.

### Architecture Score (1-10)

#### 10/10 - Exemplary
- ✅ Perfect separation of concerns (UI/logic/data separate)
- ✅ Follows framework conventions exactly (Next.js App Router patterns)
- ✅ Proper use of Server/Client Components
- ✅ Clean dependency flow (no circular dependencies)
- ✅ Reusable abstractions (DRY principle followed)
- ✅ Scalability considered (easy to add features)

#### 8-9/10 - Good
- ✅ Separation of concerns mostly correct
- ✅ Follows conventions with minor deviations
- ⚠️ 1-2 misplaced concerns (e.g., API call in a component)
- ✅ Clean dependency flow
- ✅ Mostly DRY (slight duplication acceptable)

**Deductions:**
- -1: Data fetching in Client Component (should be Server Component)
- -1: Business logic in UI component (should be in lib/)

#### 6-7/10 - Acceptable
- ⚠️ Some architectural issues but fundamentally sound
- ⚠️ Mixing Server/Client patterns inconsistently
- ⚠️ Some duplicated code (3-4 instances)
- ⚠️ Dependencies unclear (hard to trace data flow)
- ✅ But: Can be refactored without rewrite

#### 4-5/10 - Needs Work
- ❌ Fundamental architectural problems
- ❌ No clear separation (everything in page.tsx)
- ❌ Ignores framework conventions
- ❌ Circular dependencies present
- ❌ Would require significant refactoring

#### 1-3/10 - Poor
- ❌ No architecture at all (spaghetti code)
- ❌ Framework misused completely
- ❌ Would be faster to rewrite than fix

### Readability Score (1-10)

#### 10/10 - Crystal Clear
- ✅ All variables/functions have descriptive names
- ✅ Complex logic has explanatory comments
- ✅ Consistent formatting throughout
- ✅ No confusing abbreviations
- ✅ Functions are single-purpose and small (<30 lines)
- ✅ Code reads like English

#### 8-9/10 - Very Clear
- ✅ Most names are descriptive
- ⚠️ 1-2 unclear variable names (e.g., `tmp`, `data`)
- ⚠️ One complex function could use comments
- ✅ Generally consistent formatting

**Deductions:**
- -1: For each unclear variable name (`data`, `tmp`, `x`)
- -1: Complex logic without comments

#### 6-7/10 - Readable with Effort
- ⚠️ Many unclear names (`d`, `arr`, `fn`)
- ⚠️ Multiple functions >50 lines
- ⚠️ Inconsistent formatting
- ⚠️ Takes 2+ minutes to understand each function

#### 4-5/10 - Hard to Read
- ❌ Cryptic variable names everywhere
- ❌ No comments on complex logic
- ❌ Functions >100 lines
- ❌ Inconsistent indentation
- ❌ Takes 5+ minutes to understand each function

#### 1-3/10 - Unreadable
- ❌ Impossible to understand without running it
- ❌ No naming convention at all
- ❌ Would be faster to rewrite

### Completeness Score (1-10)

#### 10/10 - Fully Complete
- ✅ Every feature in spec is implemented
- ✅ No TODO comments
- ✅ No placeholder code
- ✅ Error handling on all edge cases
- ✅ All user flows work end-to-end

#### 8-9/10 - Nearly Complete
- ✅ All major features implemented
- ⚠️ 1-2 minor features missing (e.g., search)
- ⚠️ 1-2 TODOs for nice-to-haves
- ✅ Core flows work perfectly
- ⚠️ Some edge cases not handled

**Deductions:**
- -1: For each missing minor feature
- -1: For each unhandled edge case in critical path

#### 6-7/10 - Mostly Complete
- ⚠️ 1 major feature missing (20% of spec)
- ⚠️ 3-5 TODOs in code
- ⚠️ Some core flows broken
- ⚠️ Multiple edge cases crash the app

#### 4-5/10 - Incomplete
- ❌ 2+ major features missing (40%+ of spec)
- ❌ Many placeholder functions
- ❌ Critical flows don't work
- ❌ TODOs everywhere (10+)

#### 1-3/10 - Barely Started
- ❌ Only scaffolding exists
- ❌ 60%+ of spec not implemented
- ❌ Nothing actually works

### Test Coverage Score (1-10)

#### 10/10 - Comprehensive
- ✅ Unit tests for all functions (80%+ coverage)
- ✅ Integration tests for all API routes
- ✅ E2E tests for critical user flows
- ✅ All tests pass consistently
- ✅ Tests are meaningful (not just boilerplate)
- ✅ Edge cases tested

#### 8-9/10 - Good Coverage
- ✅ Unit tests for most functions (60-80% coverage)
- ✅ Integration tests for main routes
- ⚠️ E2E tests missing or minimal
- ✅ Tests pass
- ⚠️ Some edge cases not tested

**Deductions:**
- -1: <70% unit test coverage
- -1: No E2E tests

#### 6-7/10 - Basic Coverage
- ⚠️ Unit tests exist but sparse (40-60% coverage)
- ⚠️ Only happy-path tests (no error cases)
- ⚠️ No integration tests
- ✅ What tests exist do pass

#### 4-5/10 - Minimal Testing
- ❌ <40% coverage
- ❌ Tests are mostly boilerplate
- ❌ Many tests fail or are skipped
- ❌ No meaningful assertions

#### 1-3/10 - No Testing
- ❌ No tests exist, or
- ❌ Only example tests from template

### User Experience Score (1-10)

#### 10/10 - Delightful
- ✅ Intuitive navigation (no confusion)
- ✅ Loading states for all async actions
- ✅ Error messages are helpful and clear
- ✅ Responsive design (mobile + desktop)
- ✅ Consistent styling
- ✅ Accessible (keyboard nav, ARIA labels)

#### 8-9/10 - Good UX
- ✅ Navigation mostly clear
- ✅ Loading states on critical actions
- ⚠️ 1-2 error messages are vague ("Error occurred")
- ✅ Responsive on desktop
- ⚠️ Mobile has minor issues (text too small)
- ⚠️ No accessibility features

**Deductions:**
- -1: Missing loading states on 1-2 actions
- -1: Generic error messages
- -1: Not mobile-responsive

#### 6-7/10 - Usable but Rough
- ⚠️ Navigation requires thinking
- ⚠️ Many actions have no loading feedback (user confused)
- ⚠️ Error messages unhelpful
- ⚠️ Desktop-only (breaks on mobile)
- ⚠️ Inconsistent styling

#### 4-5/10 - Frustrating
- ❌ Confusing UI (user clicks wrong thing)
- ❌ No feedback on actions (feels broken)
- ❌ Errors crash the app or show stack traces
- ❌ Completely breaks on mobile
- ❌ Ugly (no styling applied)

#### 1-3/10 - Unusable
- ❌ Can't complete basic tasks
- ❌ UI is broken or nonsensical

### Overall Quality Score Calculation

**Weighted average:**
```
Overall = (
  Architecture × 0.25 +
  Readability × 0.15 +
  Completeness × 0.25 +
  Test Coverage × 0.20 +
  User Experience × 0.15
)
```

**Automatic adjustments:**

**-2 points if:**
- ❌ App doesn't build (`npm run build` fails)
- ❌ Isolation violated (Moose files modified)
- ❌ Critical security flaw (exposed API keys, SQL injection)

**-1 point if:**
- ❌ Tests don't pass
- ❌ >10 lint errors
- ❌ Core feature completely missing

**+1 point if:**
- ✅ Exceeds spec (adds useful feature not requested)
- ✅ Exceptional code quality across all dimensions

### Score Interpretation

| Overall Score | Interpretation | Action |
|---------------|----------------|--------|
| **9-10** | Production-ready, would ship to client | ✅ Success! Move to new spec |
| **7-8** | Good quality, minor fixes needed | ✅ Success with notes |
| **5-6** | Acceptable, but needs work | ⚠️ Identify 2-3 key improvements |
| **3-4** | Significant issues | ❌ Major rework needed |
| **1-2** | Failed build | ❌ Debug and retry |

---

## 5. Implementation: Key Scripts

### 5.1 Cleanup Script

**File:** `scripts/cleanup-iteration.mjs`

**Purpose:** Reset environment between iterations while preserving learning data

**What it does:**
1. Deletes test project from database
2. Deletes all work orders for test project
3. Deletes branches from GitHub (moose-wo-*)
4. Closes open PRs
5. Deletes local project directory
6. Verifies cleanup completed

**What it preserves:**
- `test_iterations` table (all metrics)
- `iteration_work_order_logs` table (execution details)
- `moose_improvements` table (changes made to Moose)

**Key configuration:**
```javascript
const CONFIG = {
  test_project_name: 'multi-llm-discussion',
  test_project_path: 'C:\\dev\\multi-llm-discussion',
  github_owner: process.env.GITHUB_OWNER,
  github_repo: 'multi-llm-discussion',
  branch_prefix: 'moose-wo-',
};
```

### 5.2 Iteration Test Script

**File:** `scripts/run-iteration-test.mjs`

**Purpose:** Execute one complete iteration and capture all metrics

**Steps:**
1. Record Moose version (git commit hash)
2. Clean up from previous iteration
3. Initialize project via API
4. Decompose spec via API
5. Execute all work orders sequentially
6. Verify isolation (check Moose files unchanged)
7. Test the built app (build, test, lint)
8. Save all metrics to database

**Captures:**
- Execution time per work order
- Success/failure status
- Cost per work order
- Files modified
- Build/test/lint results
- Isolation verification

### 5.3 Supervised Improvement Loop

**File:** `scripts/supervised-improvement-loop.mjs`

**Purpose:** Main orchestration script with human approval gates

**Flow:**
```
1. Cleanup → 2. Execute → 3. Score → 4. Analyze
              ↓
5. Generate Proposals → 6. Create Report → 7. WAIT FOR APPROVAL
              ↓
[Approve] → Apply changes → Next iteration
[Edit]    → Modify proposals → Apply → Next iteration  
[Skip]    → Next iteration (no changes)
[Stop]    → Exit loop
```

**Key features:**
- Generates detailed markdown reports
- Shows exact code diffs
- Provides testing/rollback plans
- Tracks approval history
- Offers to switch to autonomous after N approvals

### 5.4 Scoring Helper Script

**File:** `scripts/score-iteration.mjs`

**Purpose:** Automated metrics to inform manual scoring

**Collects:**
- TODO count (affects Completeness)
- Test coverage % (affects Test Coverage)
- Cyclomatic complexity (affects Readability)
- Largest file size (affects Architecture)
- Component count (context)
- API route count (context)

**Suggests scores based on thresholds:**
- TODO count: 0 = 9-10, 1-2 = 8-9, 3-5 = 6-7, 6+ = 4-5
- Coverage: 80%+ = 9-10, 60-80% = 7-8, 40-60% = 5-6, <40% = 3-4
- File size: <200 lines = good, 200-300 = acceptable, 500+ = concern

---

## 6. Configuration Files

### 6.1 Environment Variables

**File:** `.env.local` (Moose's .env, not the test project's)

```bash
# Supabase
NEXT_PUBLIC_SUPABASE_URL=https://veofqiywppjsjqfqztft.supabase.co
NEXT_PUBLIC_SUPABASE_ANON_KEY=<your-anon-key>
SUPABASE_SERVICE_ROLE_KEY=<your-service-key>

# AI APIs
ANTHROPIC_API_KEY=<your-key>
OPENAI_API_KEY=<your-key>

# GitHub
GITHUB_TOKEN=<personal-access-token>
GITHUB_OWNER=<your-org-or-username>

# Optional: Claude Code command
CLAUDE_CODE_COMMAND=claude-code
```

### 6.2 Test Specification

**File:** `test-specs/MULTI_LLM_DISCUSSION_SPEC.md`

This is the spec that will be built repeatedly. It should be:
- Well-defined with clear requirements
- Representative of typical projects Moose will build
- Complex enough to test all capabilities
- Not too large (aim for 10-15 work orders)

**Recommended complexity:**
- 3-5 main features
- Database integration
- User authentication
- Real-time updates
- API routes
- UI components
- Tests required

---

## 7. Detailed Report Structure

### Improvement Report Template

**Generated for each iteration needing improvements**

**Sections:**

1. **Iteration Summary** - Scores, metrics, execution details
2. **Analysis Summary** - What went wrong and why
3. **Issues Identified** - Categorized list with priorities
4. **Proposed Improvements** - Detailed proposals with:
   - File to change
   - Exact code diff
   - Rationale
   - Testing plan
   - Rollback plan
   - Estimated time
   - Risks
5. **Recommended Action Plan** - Ordered by priority
6. **Approval Checklist** - For human review
7. **Next Steps** - Commands to approve/edit/skip

**Example proposal structure:**
```markdown
### Proposal 1: Add error handling emphasis to proposer prompts

**Addresses Issue:** Missing error handling in API routes

**Priority:** high (Expected Impact: +2 to Completeness, +1 to UX)

**Changes Required:**

#### File: `src/lib/proposer-service.ts`

**What to change:**
Add error handling requirement to system prompt

**Code diff preview:**
```diff
  const systemPrompt = `
  You are an expert software engineer.
  
+ CRITICAL: Always include comprehensive error handling.
+ - Wrap all API calls in try-catch blocks
+ - Provide user-friendly error messages
```

**Why this helps:**
Proposer currently doesn't emphasize error handling...

**Testing Plan:**
1. Run iteration with new prompt
2. Verify generated code has try-catch blocks
3. Check Completeness score improves to 7-8/10

**Rollback Plan:**
`git revert <commit-hash>`

**Estimated Time:** 5 minutes

**Risks:** May increase response length by 10-15%
```

---

## 8. Implementation Checklist for Claude Code

### Phase 1: Database Setup (30 min)

- [ ] Run SQL schema in Supabase SQL Editor
- [ ] Verify all 3 tables created
- [ ] Test helper functions work
- [ ] Add indexes for performance:
  ```sql
  CREATE INDEX idx_iterations_number ON test_iterations(iteration_number);
  CREATE INDEX idx_wo_logs_iteration ON iteration_work_order_logs(iteration_id);
  CREATE INDEX idx_improvements_from ON moose_improvements(from_iteration_id);
  ```

### Phase 2: Cleanup Script (1-2 hours)

- [ ] Create `scripts/cleanup-iteration.mjs`
- [ ] Implement database cleanup
- [ ] Implement GitHub cleanup (branches, PRs)
- [ ] Implement filesystem cleanup
- [ ] Add verification step
- [ ] Test cleanup works correctly
- [ ] Test cleanup is idempotent (can run multiple times safely)

### Phase 3: Iteration Test Script (2-3 hours)

- [ ] Create `scripts/run-iteration-test.mjs`
- [ ] Implement project initialization
- [ ] Implement decomposition
- [ ] Implement work order execution loop
- [ ] Add metrics collection
- [ ] Add isolation verification
- [ ] Add build/test/lint checks
- [ ] Test full iteration works

### Phase 4: Scoring System (2-3 hours)

- [ ] Create `docs/SCORING_RUBRICS.md` with full rubrics
- [ ] Create `scripts/score-iteration.mjs` for automated metrics
- [ ] Implement TODO counting
- [ ] Implement test coverage extraction
- [ ] Implement complexity calculation
- [ ] Implement file size analysis
- [ ] Test scoring suggestions are accurate

### Phase 5: Supervised Loop (4-5 hours)

- [ ] Create `scripts/supervised-improvement-loop.mjs`
- [ ] Implement main loop with states
- [ ] Implement scoring integration (call Claude Code)
- [ ] Implement analysis integration (call Claude Code)
- [ ] Implement proposal generation (call Claude Code)
- [ ] Create report generator
- [ ] Implement interactive approval prompts
- [ ] Implement approval → apply changes flow
- [ ] Implement edit mode
- [ ] Add autonomous mode switch offer
- [ ] Test full supervised loop

### Phase 6: Report Generation (2 hours)

- [ ] Create report template
- [ ] Implement markdown formatting
- [ ] Add code diff rendering
- [ ] Add approval checklist
- [ ] Test report readability
- [ ] Add "view in terminal" option

### Phase 7: Improvement Application (2 hours)

- [ ] Implement change application logic
- [ ] Add git commit for each change
- [ ] Add build verification after each change
- [ ] Implement rollback on failure
- [ ] Save improvements to database
- [ ] Test changes are applied correctly

### Phase 8: Integration & Testing (2-3 hours)

- [ ] Run full supervised loop end-to-end
- [ ] Verify cleanup works between iterations
- [ ] Verify reports are generated correctly
- [ ] Verify proposals are actionable
- [ ] Test approve/edit/skip/stop flows
- [ ] Test rollback works
- [ ] Test autonomous mode switch

### Phase 9: Documentation (1 hour)

- [ ] Create `docs/SUPERVISED_LOOP_GUIDE.md`
- [ ] Document how to start the loop
- [ ] Document how to review reports
- [ ] Document how to edit proposals
- [ ] Document how to interpret scores
- [ ] Add troubleshooting section

---

## 9. Usage Instructions

### Starting the Supervised Loop

```bash
# 1. Ensure environment is clean
node scripts/cleanup-iteration.mjs

# 2. Start supervised loop
node scripts/supervised-improvement-loop.mjs

# 3. Wait for first iteration to complete
# (Creates project, decomposes, executes work orders, scores)

# 4. Review generated report in reports/ directory

# 5. Make decision at prompt:
#    [a] approve - Apply all changes
#    [e] edit - Modify proposals first
#    [s] skip - Move to next iteration without changes
#    [x] stop - Exit loop
#    [v] view - Show report in terminal
```

### Reviewing Reports

1. Open report file (will be in `reports/iteration-N-improvements-*.md`)
2. Read **Iteration Summary** - understand current quality
3. Read **Analysis Summary** - understand root causes
4. Review **Proposed Improvements** - check each proposal:
   - Does the file path make sense?
   - Is the diff correct and safe?
   - Does the rationale explain why this helps?
   - Is the expected impact realistic?
5. Check **Risks & Considerations**
6. Make decision: approve, edit, skip, or stop

### Editing Proposals

If you choose "edit" mode:
1. Report opens in your default editor (VS Code)
2. Modify proposals as needed:
   - Change priority
   - Adjust code diffs
   - Add/remove proposals
   - Update testing plans
3. Save and close editor
4. Loop will parse edited report and apply changes

**Note:** Current implementation parses markdown back to JSON. For complex edits, modify `temp/proposals.json` directly.

### Switching to Autonomous Mode

After ~5 successful iterations where you approved changes, the system will ask:

```
You've approved 5 improvements successfully.
Would you like to switch to AUTONOMOUS mode? (y/n):
```

If you say yes:
- Loop continues without human approval
- Reports still generated but not reviewed
- Can press Ctrl+C to stop at any time
- All changes still committed with descriptive messages
- Can review git history later

**Recommended:** Stay in supervised mode for at least 10 iterations to build confidence.

---

## 10. Key Design Decisions Explained

### Why Supervised Mode First?

**Decision:** Human approves all changes initially, with option to go autonomous later

**Rationale:**
- Builds confidence in the system gradually
- Allows learning what types of improvements work
- Prevents runaway changes if analysis is wrong
- Easy to stop and adjust if something unexpected happens
- Low risk - can always rollback

**Alternative considered:** Fully autonomous from start - rejected because too risky without validation

### Why Local Execution?

**Decision:** Run Moose locally during testing, not on cloud

**Rationale:**
- Aider works identically (it's a CLI tool)
- Git operations work the same
- Can watch files change in real-time
- Instant feedback, no deployment delays
- Easy to debug and manually intervene
- No cloud costs during testing

**Alternative considered:** Deploy to Railway/Render first - rejected because unnecessary complexity

### Why Same Spec Repeatedly?

**Decision:** Build "Multi-LLM Discussion" multiple times, not different specs

**Rationale:**
- Isolates Moose improvements (removes spec complexity as variable)
- Can directly compare iterations (same inputs)
- Can measure if changes actually helped
- Builds muscle memory for this type of project
- Once quality target hit, proves Moose can build production apps

**Alternative considered:** Different specs each time - rejected because can't measure improvement

### Why Cleanup Between Iterations?

**Decision:** Delete all artifacts but preserve metrics

**Rationale:**
- Starts each iteration from clean slate
- Prevents artifacts from previous iterations affecting results
- Avoids accumulating garbage (branches, directories, DB records)
- But keeps learning data (scores, analysis, improvements)
- Makes iterations comparable (same starting conditions)

**What's preserved:**
- ✅ test_iterations (all scores and metrics)
- ✅ iteration_work_order_logs (execution details)
- ✅ moose_improvements (what changed)

**What's deleted:**
- ❌ projects table record
- ❌ work_orders table records
- ❌ GitHub branches
- ❌ GitHub PRs (closed)
- ❌ Local project directory

### Why Detailed Reports?

**Decision:** Generate comprehensive markdown reports with code diffs

**Rationale:**
- Human needs context to make approval decision
- Seeing exact code changes builds confidence
- Testing/rollback plans reduce risk
- Can learn from analysis even if not approving
- Reports become documentation of improvement journey

**Alternative considered:** Simple "apply these changes y/n?" - rejected because insufficient context

---

## 11. Troubleshooting Guide

### Issue: Cleanup fails with "project not found"

**Cause:** Project was already deleted manually or doesn't exist

**Fix:** This is expected on first run. Script should handle gracefully with "ℹ️ No test project found"

**Action:** No action needed, continue

---

### Issue: GitHub API rate limit hit

**Symptom:** Error "API rate limit exceeded"

**Cause:** Made too many requests (5000/hour limit)

**Fix:**
1. Wait 1 hour for rate limit to reset
2. Or use GitHub App authentication (higher limits)

**Prevention:** Add rate limit checking before API calls

---

### Issue: Iteration test fails with "git not found"

**Cause:** Git is not in PATH or Aider can't find it

**Fix:**
1. Verify git installed: `git --version`
2. Add git to PATH
3. Restart terminal

---

### Issue: Claude Code returns errors during scoring

**Symptom:** "temp/scoring-results.json not found"

**Cause:** Claude Code didn't complete task or encountered error

**Fix:**
1. Check temp/ directory for error logs
2. Verify Claude Code is authenticated: `claude-code --version`
3. Check API key is valid
4. Try running Claude Code command manually

---

### Issue: Build succeeds but tests fail in iteration

**Symptom:** `builds_successfully: true` but `tests_pass: false`

**Cause:** This is expected - it's why we're iterating

**Action:**
1. Review which tests failed in report
2. Analysis should identify why tests failed
3. Proposals should address test failures
4. Next iteration should have better test coverage

---

### Issue: Moose files were modified (isolation violated)

**Symptom:** `moose_files_modified: true` in iteration results

**Cause:** CRITICAL - Aider executed in wrong directory

**Fix:**
1. Stop loop immediately
2. Check git status in Moose directory
3. Rollback any changes: `git reset --hard HEAD`
4. Verify project paths in orchestrator-service.ts
5. Add safety check to prevent self-modification

**Prevention:** Add this check before Aider execution:
```javascript
if (workingDirectory === process.cwd()) {
  throw new Error('Refusing to execute in Moose directory!');
}
```

---

### Issue: Proposals don't address the actual problems

**Symptom:** Improvements applied but score doesn't increase

**Cause:** Analysis was incorrect or proposals were off-target

**Action:**
1. Review analysis details in database
2. Check if evidence supports the conclusion
3. Edit proposals before approving next time
4. May need to refine analysis prompts

---

### Issue: Too many proposals generated (10+)

**Symptom:** Report has 10+ proposals, overwhelming to review

**Cause:** Analysis found many issues

**Action:**
1. Select top 2-3 highest priority proposals
2. Use "edit" mode to remove low-priority proposals
3. Apply high-priority changes first
4. Address remaining issues in next iteration

---

## 12. Success Metrics & Stopping Criteria

### When to Stop Iterating

**Target:** Overall quality score >= 8/10 for **3 consecutive iterations**

**Why 3 consecutive?** Ensures consistency, not just lucky once

**Additional criteria (all must be true):**
- ✅ Build succeeds (npm run build)
- ✅ Tests pass (npm test)
- ✅ Lint clean (<5 errors)
- ✅ Isolation verified (Moose unchanged)
- ✅ All work orders succeeded

### Intermediate Milestones

**Iteration 1-3:** Learning phase
- Expected scores: 5-6/10
- Goal: Understand common failure patterns
- Action: Approve obvious improvements

**Iteration 4-7:** Improvement phase
- Expected scores: 6-7/10
- Goal: Address major issues
- Action: Be selective with proposals, focus on high-impact

**Iteration 8-12:** Refinement phase
- Expected scores: 7-8/10
- Goal: Polish and consistency
- Action: Focus on edge cases and test coverage

**Iteration 13+:** Production-ready
- Expected scores: 8-9/10
- Goal: Maintain quality consistently
- Action: Minor tweaks only

### What to Track

**Primary metrics:**
- Overall quality score trend (should go up)
- Work order success rate (should go up)
- Consecutive successful iterations (should reach 3)

**Secondary metrics:**
- Execution time (should stabilize around 15-20 min)
- Cost per iteration (should stabilize around $1-2)
- Number of proposals per iteration (should decrease over time)

**Red flags:**
- ⚠️ Score decreasing (changes made things worse)
- ⚠️ Same issues appearing repeatedly (improvements not working)
- ⚠️ Isolation violated (critical bug)
- ⚠️ Cost spiking >$5 per iteration (inefficiency)

---

## 13. Expected Timeline

**Conservative estimate:**

| Phase | Duration | Description |
|-------|----------|-------------|
| Setup | 0.5 days | Database schema, scripts, rubrics |
| First iteration | 1 hour | Run, review, understand |
| Iterations 2-5 | 2-3 days | Major improvements, learning |
| Iterations 6-10 | 2-3 days | Refinement, consistency |
| Iterations 11-15 | 1-2 days | Final polish |
| **Total** | **5-7 days** | To production-ready Moose |

**Aggressive estimate (if issues are minor):**
- 3-4 days to reach quality target

**Realistic expectation:**
- Plan for 1 week
- May discover unexpected issues requiring investigation
- May need to refine scoring or proposals
- May hit technical blockers

---

## 14. Next Steps for Claude Code

### Immediate Actions (Day 1)

1. **Read this document completely**
2. **Set up database schema** - Run SQL in Supabase
3. **Create cleanup script** - Critical for iteration flow
4. **Test cleanup works** - Run it manually first
5. **Create scoring rubrics document** - Reference for scoring
6. **Verify environment ready:**
   - Git works
   - GitHub API accessible
   - Supabase connected
   - Claude Code installed

### Build Phase (Days 2-3)

1. **Implement iteration test script** - Core execution logic
2. **Test one iteration** - Does it complete without errors?
3. **Implement scoring helper** - Automated metrics
4. **Create report template** - Markdown structure
5. **Test report generation** - Is it readable and actionable?

### Integration Phase (Days 4-5)

1. **Implement supervised loop** - Main orchestrator
2. **Test approval flow** - Does prompt work correctly?
3. **Implement proposal generation** - Claude Code creates proposals
4. **Test improvement application** - Do changes get applied correctly?
5. **Add rollback safety** - Critical for confidence

### Validation Phase (Day 6)

1. **Run full supervised loop** - End-to-end test
2. **Review first report** - Is it actionable?
3. **Approve a proposal** - Does it apply correctly?
4. **Run second iteration** - Did score improve?
5. **Document any issues** - For troubleshooting guide

### Production Phase (Day 7)

1. **Run supervised loop to completion** - Until quality target met
2. **Generate final report** - Document the journey
3. **Review all git commits** - What changed in Moose?
4. **Test Moose on different spec** - Does improvement transfer?
5. **Document learnings** - For future improvements

---

## 15. Important Notes

### On Scoring

- Be **objective** - use the rubric criteria, not gut feeling
- Be **consistent** - same score for same quality across iterations
- Be **harsh initially** - easier to improve from 5→8 than 7→8
- **Document reasoning** - write why you gave each score

### On Proposals

- **Specific is better** - "Add try-catch blocks" < "Add try-catch to all fetch() calls in src/app/api/"
- **One change at a time** - Don't combine 5 changes in one proposal
- **Test after each** - Don't stack untested changes
- **Include rollback** - Always have an undo plan

### On Approval

- **Don't approve everything** - Be selective, focus on high-impact
- **Edit liberally** - Proposals are starting points, not final
- **Skip when uncertain** - Better to skip than apply bad changes
- **Stop if confused** - Don't proceed if you don't understand

### On Iteration Speed

- **Quality > Speed** - Better to take 2 weeks and get it right
- **Don't rush approvals** - Take time to understand each proposal
- **Pause between iterations** - Let metrics stabilize
- **Review regularly** - Check if improvements are actually helping

---

## 16. Critical Success Factors

### What Must Go Right

1. ✅ **Cleanup works perfectly** - Can't iterate if environment is polluted
2. ✅ **Isolation maintained** - Moose must never modify itself during execution
3. ✅ **Scoring is objective** - Can't improve without accurate measurement
4. ✅ **Proposals are actionable** - Must be specific enough to implement
5. ✅ **Changes are tested** - Must verify build still works after each change

### What Could Go Wrong

1. ⚠️ **Analysis is wrong** - Proposes changes that don't help
2. ⚠️ **Changes break Moose** - Introduces bugs into orchestrator
3. ⚠️ **Scores don't improve** - Changes don't address root causes
4. ⚠️ **Iterations diverge** - Each iteration produces different issues
5. ⚠️ **Loop runs forever** - Never reaches quality target

### Mitigation Strategies

- **Supervised mode** - Human catches wrong analysis before damage done
- **Build verification** - Catches broken changes immediately
- **Rollback capability** - Can undo bad changes
- **Max iteration limit** - Prevents infinite loops
- **Git history** - Can review and revert anytime

---

## 17. Final Checklist Before Starting

### Environment Ready

- [ ] Supabase database schema applied
- [ ] All tables created and verified
- [ ] Environment variables configured
- [ ] GitHub token has required permissions
- [ ] Claude Code authenticated and working
- [ ] Git configured (user.name, user.email)

### Scripts Created

- [ ] `scripts/cleanup-iteration.mjs`
- [ ] `scripts/run-iteration-test.mjs`
- [ ] `scripts/score-iteration.mjs`
- [ ] `scripts/supervised-improvement-loop.mjs`

### Documentation Ready

- [ ] `docs/SCORING_RUBRICS.md`
- [ ] `test-specs/MULTI_LLM_DISCUSSION_SPEC.md`
- [ ] This implementation guide reviewed

### Test Ready

- [ ] Can run cleanup script successfully
- [ ] Can initialize a test project
- [ ] Can decompose a spec
- [ ] Can execute one work order
- [ ] GitHub repo exists for multi-llm-discussion

### Understanding Confirmed

- [ ] Understand supervised vs autonomous mode
- [ ] Know how to review reports
- [ ] Know how to approve/edit/skip proposals
- [ ] Understand what cleanup does and doesn't delete
- [ ] Know stopping criteria (8/10 for 3 iterations)

---

**When all checkboxes are complete, you're ready to run:**

```bash
node scripts/supervised-improvement-loop.mjs
```

**Good luck! This is an ambitious but achievable goal. The supervised mode will keep you safe while Moose improves itself. Trust the process, be patient, and document your learnings.**

---

*End of Implementation Guide*