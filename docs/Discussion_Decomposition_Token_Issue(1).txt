## 📋 Discussion Document: Implementing Staged Output Strategy for Architect

**Date:** 2025-10-07  
**Topic:** Overcoming 4K Token Output Limit via Batched Decomposition  
**Participants:** User, Claude Code (strategic discussion partner)  
**Status:** Planning phase - seeking implementation approach

---

## 🎯 Problem Statement

**Current Constraint:** Claude API has a hard 4,000 token output limit per response.

**Impact on Moose:**
- Current limit: ~20-23 work orders per decomposition (at 170 tokens/WO)
- Multi-LLM Discussion App needs: ~35 work orders
- Future enterprise projects need: 100-1,000 work orders
- **We've hit the ceiling and need a different architecture**

**Current Status:**
- ✅ Phase 1 defensive constructors implemented and working
- ✅ 20 work order limit set (fills 85% of token capacity)
- ❌ Still insufficient for our test case (Multi-LLM App)
- ❌ Doesn't scale to enterprise-level projects

---

## 💡 Proposed Solution: Staged Output with Batching

Instead of asking Claude to generate all work orders at once, break the decomposition into **multiple sequential API calls** that Moose orchestrates and combines.

### **Core Pattern:**

```
Single Call (Current - FAILS):
┌─────────────────────────────┐
│ "Generate 35 work orders"   │
│ → 5,950 tokens needed       │
│ → Hits 4K limit             │
│ → Truncated/incomplete ❌   │
└─────────────────────────────┘

Batched Calls (Proposed - WORKS):
┌─────────────────────────────┐
│ "Generate WO 1-10"          │
│ → 1,870 tokens ✅           │
└─────────────────────────────┘
         ↓
┌─────────────────────────────┐
│ "Generate WO 11-20"         │
│ (context: WO 1-10 summary)  │
│ → 1,870 tokens ✅           │
└─────────────────────────────┘
         ↓
┌─────────────────────────────┐
│ "Generate WO 21-30"         │
│ (context: WO 1-20 summary)  │
│ → 1,870 tokens ✅           │
└─────────────────────────────┘
         ↓
┌─────────────────────────────┐
│ "Generate WO 31-35"         │
│ (context: WO 1-30 summary)  │
│ → 900 tokens ✅             │
└─────────────────────────────┘
         ↓
    Moose combines → 35 WOs total
```

---

## 🔍 Three Implementation Approaches

### **Option 1: Explicit Batching (RECOMMENDED)**

**How it works:**
1. Estimation call determines total work order count needed
2. Moose calculates number of batches (e.g., 35 ÷ 10 = 4 batches)
3. Each batch generates 10 work orders sequentially
4. Each batch receives compressed summary of previous batches
5. Moose combines all batches into final decomposition

**Key Code Structure:**
```typescript
async decomposeWithAutoBatching(spec: TechnicalSpec): Promise<Decomposition> {
  // Step 1: Estimate complexity
  const estimate = await this.estimateComplexity(spec);
  // "This spec needs approximately 35 work orders"
  
  // Step 2: Decide approach
  if (estimate.workOrderCount <= 20) {
    return this.decomposeInSingleCall(spec); // Fast path
  }
  
  // Step 3: Batch decomposition
  const batchSize = estimate.batchSize; // Usually 10
  const numBatches = Math.ceil(estimate.workOrderCount / batchSize);
  const allWorkOrders: WorkOrder[] = [];
  
  for (let i = 0; i < numBatches; i++) {
    const startIdx = i * batchSize + 1;
    const endIdx = Math.min((i + 1) * batchSize, estimate.workOrderCount);
    
    const batch = await this.generateBatch(
      spec,
      startIdx,
      endIdx,
      allWorkOrders // Previous WOs for context
    );
    
    allWorkOrders.push(...batch);
  }
  
  // Step 4: Validate dependencies across batches
  return this.validateAndCombine(allWorkOrders);
}
```

**Pros:**
- ✅ Simple and predictable
- ✅ Claude understands explicit batch instructions well
- ✅ Can scale to unlimited work orders
- ✅ Previous context helps maintain coherence
- ✅ Each batch is independent (easier debugging)

**Cons:**
- ⚠️ Multiple API calls = higher cost (4× for Multi-LLM App)
- ⚠️ Requires good estimation upfront
- ⚠️ Context summary grows with each batch
- ⚠️ Potential consistency issues across batches

**Cost for Multi-LLM App (35 WOs):**
- Estimation call: $0.01 (15 seconds)
- 4 batches: $0.40 (2 minutes)
- **Total: $0.41, ~2.5 minutes**

---

### **Option 2: Continuation Pattern**

**How it works:**
1. Initial request asks Claude to decompose
2. Claude generates as many WOs as fit in 4K tokens
3. Claude signals "CONTINUE: X more needed"
4. Moose requests continuation
5. Repeat until Claude signals "COMPLETE"

**Key Code Structure:**
```typescript
async decomposeWithContinuation(spec: TechnicalSpec): Promise<Decomposition> {
  const conversationHistory = [];
  const allWorkOrders: WorkOrder[] = [];
  let isComplete = false;
  
  // Initial request
  conversationHistory.push({
    role: 'user',
    content: `Decompose this spec. If you need multiple responses, 
              end with "CONTINUE: X more needed", else "COMPLETE"`
  });
  
  while (!isComplete) {
    const response = await this.claude.messages.create({
      messages: conversationHistory
    });
    
    const batch = this.extractWorkOrders(response.content[0].text);
    allWorkOrders.push(...batch);
    
    if (response.content[0].text.includes('CONTINUE:')) {
      conversationHistory.push({ role: 'assistant', content: response.content[0].text });
      conversationHistory.push({ role: 'user', content: 'Please continue' });
    } else {
      isComplete = true;
    }
  }
  
  return { work_orders: allWorkOrders };
}
```

**Pros:**
- ✅ Natural conversation flow
- ✅ Claude maintains context automatically
- ✅ Adaptive (Claude decides when to continue)
- ✅ Better coherence (full conversation history)

**Cons:**
- ⚠️ Multi-turn conversation = higher cost
- ⚠️ Context window grows with full message history
- ⚠️ Harder to predict total cost/time
- ⚠️ Claude might not signal continuation reliably
- ⚠️ More complex error recovery

**Cost for Multi-LLM App (35 WOs):**
- 4 turns @ $0.12 each
- **Total: ~$0.50, ~3 minutes**

---

### **Option 3: Staged Hierarchical**

**How it works:**
1. Extract high-level features (single call)
2. For each feature, generate work orders
3. Complex features use batching internally
4. Simple features stay single-call

**Key Code Structure:**
```typescript
async decomposeStaged(spec: TechnicalSpec): Promise<Decomposition> {
  // Step 1: Extract features
  const features = await this.extractFeatures(spec);
  // Output: ["Infrastructure", "Core Systems", "UI", "Testing"]
  
  const allWorkOrders: WorkOrder[] = [];
  
  // Step 2: Decompose each feature
  for (const feature of features) {
    if (this.isSimpleFeature(feature)) {
      // Single call for simple features
      const wos = await this.decomposeFeature(feature, spec);
      allWorkOrders.push(...wos);
    } else {
      // Batched for complex features
      const wos = await this.decomposeFeatureInBatches(feature, spec);
      allWorkOrders.push(...wos);
    }
  }
  
  return { work_orders: allWorkOrders };
}
```

**Pros:**
- ✅ Only batches when needed (adaptive)
- ✅ Natural hierarchical organization
- ✅ Each feature is coherent unit
- ✅ Efficient for mixed complexity

**Cons:**
- ⚠️ Most complex orchestration logic
- ⚠️ Variable cost (hard to predict)
- ⚠️ Harder to implement and debug
- ⚠️ More API calls overall

**Cost for Multi-LLM App (35 WOs):**
- Feature extraction: $0.10
- 6 feature decompositions: $0.35
- **Total: ~$0.45, ~2.5 minutes**

---

## 📊 Comparison Matrix

| Aspect | Single Call (Current) | Explicit Batching | Continuation | Staged Hierarchical |
|--------|----------------------|-------------------|--------------|---------------------|
| **Max Work Orders** | 20-23 | Unlimited | Unlimited | Unlimited |
| **API Calls (35 WOs)** | 1 (fails) | 5 (1 estimate + 4 batches) | 4-5 | 7-8 |
| **Cost (35 WOs)** | $0.15 | $0.41 | $0.50 | $0.45 |
| **Time (35 WOs)** | 30s | 2.5min | 3min | 2.5min |
| **Predictability** | ✅ Perfect | ✅ Good | ⚠️ Variable | ⚠️ Variable |
| **Complexity** | ✅ Simple | 🟡 Moderate | 🟡 Moderate | 🔴 Complex |
| **Coherence** | ✅ Perfect | 🟡 Good | ✅ Excellent | 🟡 Good |
| **Scalability** | ❌ Limited | ✅ Excellent | ✅ Excellent | ✅ Excellent |
| **Debuggability** | ✅ Easy | ✅ Easy | 🟡 Moderate | 🔴 Hard |

---

## 🎯 Recommendation: Explicit Batching

**Why this is the best choice for Moose:**

### **1. Balances All Concerns**
- Predictable cost and time (estimation call tells you upfront)
- Simple to implement and debug (each batch is independent)
- Scales to enterprise projects (100-1,000 WOs)
- Maintains reasonable coherence (compressed context)

### **2. Adaptive Strategy**
```typescript
// Small projects stay fast and cheap
if (estimatedWOs <= 20) {
  return singleCallDecompose(); // $0.15, 30s
}

// Large projects use batching
else {
  return batchedDecompose(); // $0.40-1.00, 2-5min
}
```

### **3. Real Cost Impact**
- Multi-LLM App (35 WOs): $0.41 vs $0.15 (2.7× increase)
- But still 39,000× cheaper than human ($16,000)
- Time increase: 2.5min vs 30s (5× slower)
- But execution still takes 6-7 hours (unchanged)

### **4. Implementation Simplicity**
Three clear components:
1. **Estimator**: Quick call to determine WO count
2. **Batcher**: Generate WOs in groups of 10
3. **Validator**: Ensure dependencies are consistent

---

## 🛠️ Implementation Plan

### **Phase 1: Core Batching (This Week)**

**Task 1.1: Create Complexity Estimator**
```typescript
// src/services/ComplexityEstimator.ts
class ComplexityEstimator {
  async estimate(spec: TechnicalSpec): Promise<ComplexityEstimate> {
    // Small API call (~500 tokens out)
    // Returns: { workOrderCount, batchSize, phases }
    // Cost: ~$0.01
  }
}
```

**Task 1.2: Implement Batched Decomposition**
```typescript
// src/lib/architect-service.ts
async decomposeWithAutoBatching(spec: TechnicalSpec): Promise<Decomposition> {
  const estimate = await this.estimator.estimate(spec);
  
  if (estimate.workOrderCount <= 20) {
    return this.decomposeInSingleCall(spec);
  }
  
  return this.decomposeInBatches(spec, estimate);
}

private async decomposeInBatches(...): Promise<Decomposition> {
  // Generate batches sequentially
  // Each batch gets compressed summary of previous
  // Combine and validate
}
```

**Task 1.3: Add Dependency Validator**
```typescript
// src/services/DependencyValidator.ts
class DependencyValidator {
  validate(workOrders: WorkOrder[]): ValidationResult {
    // Check for circular dependencies
    // Check for missing dependencies
    // Verify numbering consistency
    // Return issues + auto-fix suggestions
  }
}
```

**Estimated Effort:** 2-3 days

---

### **Phase 2: Testing & Refinement (Next Week)**

**Task 2.1: Test Multi-LLM App**
- Run batched decomposition on 35-WO spec
- Verify coherence across batches
- Measure actual cost and time
- Validate dependencies are correct

**Task 2.2: Edge Case Testing**
- Very small specs (3-5 WOs): Should use single call
- Medium specs (15-20 WOs): Should use single call
- Large specs (30-50 WOs): Should use batching
- Huge specs (100+ WOs): Should handle gracefully

**Task 2.3: Optimization**
- Tune batch size (8? 10? 12?)
- Optimize context compression
- Improve estimation accuracy

**Estimated Effort:** 1-2 days

---

### **Phase 3: Production Readiness (Week After)**

**Task 3.1: Monitoring & Logging**
- Log batch count and timing
- Track cost per decomposition
- Alert on unexpected behavior

**Task 3.2: User Experience**
- Progress indicators during batching
- Cost preview before decomposition
- Ability to cancel mid-batch

**Task 3.3: Documentation**
- Update Architect documentation
- Add batching explanation
- Create troubleshooting guide

**Estimated Effort:** 1-2 days

---

## 💰 Cost Analysis

### **Multi-LLM Discussion App (35 WOs)**

**With Single Call (Current - FAILS):**
- Decomposition: N/A (doesn't work)

**With Batching (Proposed):**
- Estimation: $0.01 (15s)
- Batch 1 (WO 1-10): $0.10 (30s)
- Batch 2 (WO 11-20): $0.10 (30s)
- Batch 3 (WO 21-30): $0.10 (30s)
- Batch 4 (WO 31-35): $0.09 (25s)
- **Total: $0.40, 2.5 minutes**

**Full Build Cost:**
- Decomposition: $0.40
- Execution (6-7 hours): $15-18
- **Total: $15.40-18.40**

**Compare to Human:**
- Human: $8,000-12,000 (80-120 hours @ $100/hr)
- Moose: $18
- **Savings: 444× cheaper** 🎉

---

### **Enterprise Project (100 WOs)**

**With Batching:**
- Estimation: $0.01
- 10 batches: $1.00
- **Total: $1.01, 5 minutes**

**Still incredibly cost-effective!**

---

## 🚧 Risks & Mitigations

### **Risk 1: Coherence Across Batches**
**Issue:** Work orders in batch 4 might not align with batch 1

**Mitigation:**
- Provide compressed summary of all previous WOs to each batch
- Validation pass checks dependencies
- Estimation call establishes overall structure

### **Risk 2: Context Window Growth**
**Issue:** Summary of previous WOs grows with each batch

**Mitigation:**
- Ultra-compressed format: `WO-1: Title only`
- Only include WO number and title (not full description)
- Example: 100 WOs × 50 chars = 5,000 chars = 1,250 tokens (acceptable)

### **Risk 3: Estimation Inaccuracy**
**Issue:** Estimate says "25 WOs" but actually needs 35

**Mitigation:**
- Allow Claude to add "spillover" WOs if needed
- Final validation pass can request additional WOs
- Estimation errs on high side (overestimate is safe)

### **Risk 4: Increased Cost**
**Issue:** 4× cost increase might exceed budget

**Mitigation:**
- Still well under $100/day budget
- Only use batching when needed (≤20 WOs stay single-call)
- Cost is still 440× cheaper than human

---

## ❓ Questions for Claude Code

### **Technical Questions:**

1. **Where should the batching logic live?**
   - Option A: Inside `architect-service.ts` (centralized)
   - Option B: New `BatchedArchitectService.ts` (separate)
   - Option C: As a wrapper around existing service

2. **How should we handle the context summary?**
   - Option A: Just titles (minimal)
   - Option B: Titles + dependencies (moderate)
   - Option C: Titles + descriptions + dependencies (full)

3. **What's the right batch size?**
   - Option A: Fixed 10 WOs per batch
   - Option B: Adaptive based on complexity
   - Option C: Let estimation call decide

4. **Should estimation be required or optional?**
   - Option A: Always run estimation first
   - Option B: Only estimate if spec looks complex
   - Option C: Allow manual override (user sets batch count)

### **Strategic Questions:**

1. **Should we implement all three options and let users choose?**
2. **Or commit to Explicit Batching as the One True Way?**
3. **Should batching be opt-in (flag) or automatic (always)?**
4. **What's the failure mode if estimation is wildly wrong?**

---

## 🎯 Proposed Decision

**I recommend we:**

1. **Implement Explicit Batching only** (not all three approaches)
2. **Make it automatic** (no user flags needed)
3. **Use adaptive batch size** from estimation call
4. **Start with Multi-LLM App as test case**
5. **Iterate based on real results**

**This gives us:**
- ✅ Solution to 4K token limit
- ✅ Scales to enterprise projects
- ✅ Reasonable cost increase (2-4×)
- ✅ Manageable complexity
- ✅ Path to 100-1,000 WO projects

---

## 📝 Next Steps

**If Claude Code agrees with this approach:**

1. **Discuss implementation details** (questions above)
2. **Create detailed technical spec** for batching system
3. **Implement Phase 1** (core batching)
4. **Test with Multi-LLM App** (validate it works)
5. **Iterate and refine** based on results

**If Claude Code has concerns:**

1. **Discuss alternatives** (Continuation? Hierarchical?)
2. **Identify specific risks** we haven't considered
3. **Propose hybrid approaches** if pure batching has issues
4. **Adjust plan** based on technical insights

---

## 💬 Discussion Prompt for Claude Code

> "We've hit the 4K token output limit and need a way to scale beyond 20 work orders. I've outlined three approaches above, with Explicit Batching as my recommendation.
>
> **Questions for you:**
> 1. Do you agree Explicit Batching is the best approach? If not, what concerns you?
> 2. What implementation details am I missing or underestimating?
> 3. Are there technical risks I haven't considered?
> 4. Should we proceed with implementing this, or explore alternatives first?
>
> Let's discuss the architecture before we start coding. I want your engineering perspective on this."

---

**End of Discussion Document**